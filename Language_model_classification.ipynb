{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eluwNeoz3QUh"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "lqv13-EL4OSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dVApa6lR4E8r",
        "outputId": "1ccab2c1-6d09-438b-d046-56619317184f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-08381aa1-d6fa-4cdb-8ba0-31a9058b4b00\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-08381aa1-d6fa-4cdb-8ba0-31a9058b4b00\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving IMDB Dataset.csv to IMDB Dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "X07ESTCeD4BK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "\n",
        "\n",
        "task1file = df\n",
        "\n",
        "print(task1file.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_OFZZeEJiF",
        "outputId": "6f7d2b1f-54f7-4bd7-e1ba-e11f9def000d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(task1file.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFyKCODAF2oU",
        "outputId": "02d7c36a-d46a-4e14-b38a-ed3ff0e752a6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_review(text):\n",
        "    # Remove <br /> tags\n",
        "    text = text.replace('<br />', ' ')\n",
        "    # Replace multiple periods with a single space\n",
        "    text = text.replace('..', ' ').replace('...', ' ')\n",
        "    # Replace periods with a space and remove surrounding spaces\n",
        "    text = text.replace('.', ' ').replace(',', ' ')\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = ' '.join(text.split())\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "task1file['review'] = task1file['review'].apply(preprocess_review)\n",
        "firstele = task1file.iloc[0]['review']\n",
        "print(firstele)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esaLSENeEvnu",
        "outputId": "3d3384c5-d389-48ae-ca26-25f0b24bc397"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to many aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare forget pretty pictures painted for mainstream audiences forget charm forget romance oz doesn't mess around the first episode i ever saw struck me as so nasty it was surreal i couldn't say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice (crooked guards who'll be sold out for a nickel inmates who'll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) watching oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#tokenization for unigrram model\n",
        "\n",
        "def tokenize_unigram(text):\n",
        "    return text.split()  # Split by whitespace to get tokens\n",
        "\n",
        "# Create a list to store all tokens\n",
        "all_tokens_unigrams = []\n",
        "\n",
        "# Tokenize each review and collect tokens\n",
        "for review in task1file['review']:\n",
        "    tokens_uni = tokenize_unigram(review)\n",
        "    all_tokens_unigrams.extend(tokens_uni)\n",
        "\n",
        "token_counts_uni = Counter(all_tokens_unigrams)\n",
        "\n",
        "# Calculate total number of tokens\n",
        "total_tokens = sum(token_counts_uni.values())\n",
        "\n",
        "# Calculate probabilities for each token\n",
        "token_probabilities = {token: count / total_tokens for token, count in token_counts_uni.items()}\n",
        "\n",
        "# Display results\n",
        "N = 10  # Number of top tokens to display\n",
        "\n",
        "# Get the top N token counts\n",
        "print(f\"Top {N} Token Counts:\")\n",
        "for token, count in token_counts_uni.most_common(N):\n",
        "    print(f\"{token}: {count}\")\n",
        "\n",
        "# Get the top N token probabilities\n",
        "print(f\"\\nTop {N} Token Probabilities:\")\n",
        "for token in list(token_counts_uni.keys())[:N]:  # First N tokens in the same order\n",
        "    print(f\"{token}: {token_probabilities[token]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmY4QSLlNUch",
        "outputId": "177c2412-2d66-431b-c73b-952a90448f7b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Token Counts:\n",
            "the: 655908\n",
            "a: 319358\n",
            "and: 319242\n",
            "of: 287963\n",
            "to: 266551\n",
            "is: 209872\n",
            "in: 183939\n",
            "it: 152384\n",
            "i: 151559\n",
            "this: 148933\n",
            "\n",
            "Top 10 Token Probabilities:\n",
            "one: 0.0044\n",
            "of: 0.0251\n",
            "the: 0.0571\n",
            "other: 0.0016\n",
            "reviewers: 0.0000\n",
            "has: 0.0029\n",
            "mentioned: 0.0001\n",
            "that: 0.0118\n",
            "after: 0.0013\n",
            "watching: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_used_word(token_counts_uni):\n",
        "    # Get the most common word and its count\n",
        "    most_common_word, most_common_count = token_counts_uni.most_common(1)[0]  # Get the first element\n",
        "    return most_common_word, most_common_count\n",
        "\n"
      ],
      "metadata": {
        "id": "wzZAZwU7Nrbt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate bigrams\n",
        "all_bigrams = [(all_tokens_unigrams[i], all_tokens_unigrams[i + 1]) for i in range(len(all_tokens_unigrams) - 1)]\n",
        "\n",
        "# Count frequencies of each bigram\n",
        "bigram_counts = Counter(all_bigrams)\n",
        "\n",
        "# Calculate total number of bigrams\n",
        "total_bigrams = sum(bigram_counts.values())\n",
        "\n",
        "# Calculate probabilities for each bigram\n",
        "bigram_probabilities = {bigram: count / total_bigrams for bigram, count in bigram_counts.items()}\n",
        "\n",
        "# Display results\n",
        "N = 10  # Number of top bigrams to display\n",
        "\n",
        "# Get the top N bigram counts\n",
        "print(f\"Top {N} Bigram Counts:\")\n",
        "for bigram, count in bigram_counts.most_common(N):\n",
        "    print(f\"{bigram}: {count}\")\n",
        "\n",
        "# Get the top N bigram probabilities\n",
        "print(f\"\\nTop {N} Bigram Probabilities:\")\n",
        "for bigram in list(bigram_counts.keys())[:N]:  # First N bigrams in the same order\n",
        "    print(f\"{bigram}: {bigram_probabilities[bigram]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxd5TxgzQKVn",
        "outputId": "48e03111-ffb4-481b-b747-23dd50468a28"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Bigram Counts:\n",
            "('of', 'the'): 76422\n",
            "('in', 'the'): 49376\n",
            "('this', 'movie'): 30301\n",
            "('and', 'the'): 26017\n",
            "('is', 'a'): 25959\n",
            "('the', 'film'): 24358\n",
            "('to', 'the'): 23511\n",
            "('to', 'be'): 23148\n",
            "('the', 'movie'): 22574\n",
            "('this', 'film'): 20841\n",
            "\n",
            "Top 10 Bigram Probabilities:\n",
            "('one', 'of'): 0.0014\n",
            "('of', 'the'): 0.0067\n",
            "('the', 'other'): 0.0004\n",
            "('other', 'reviewers'): 0.0000\n",
            "('reviewers', 'has'): 0.0000\n",
            "('has', 'mentioned'): 0.0000\n",
            "('mentioned', 'that'): 0.0000\n",
            "('that', 'after'): 0.0000\n",
            "('after', 'watching'): 0.0001\n",
            "('watching', 'just'): 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(bigram_counts, last_word):\n",
        "    # Find all bigrams that start with the last_word\n",
        "    possible_bigrams = {bigram: count for bigram, count in bigram_counts.items() if bigram[0] == last_word}\n",
        "\n",
        "    # If there are no possible bigrams, return a message\n",
        "    if not possible_bigrams:\n",
        "        return \"No predictions available.\"\n",
        "\n",
        "    # Find the bigram with the highest count\n",
        "    predicted_bigram = max(possible_bigrams, key=possible_bigrams.get)\n",
        "\n",
        "    # Return the next word in the bigram\n",
        "    return predicted_bigram[1]\n",
        "\n",
        "# Example usage\n",
        "last_input_word = \"this\"  # Replace this with the last word from your input sentence\n",
        "predicted_word = predict_next_word(bigram_counts, last_input_word)\n",
        "\n",
        "print(f\"Next word after '{last_input_word}': {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6YdlydyQmKR",
        "outputId": "dc999a4d-f489-46cf-e9ea-a083570dae6e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word after 'this': movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_trigrams = [(all_tokens_unigrams[i], all_tokens_unigrams[i + 1], all_tokens_unigrams[i + 2])\n",
        "                for i in range(len(all_tokens_unigrams) - 2)]\n",
        "\n",
        "# Count frequencies of each trigram\n",
        "trigram_counts = Counter(all_trigrams)\n",
        "\n",
        "# Display the first 10 trigram counts\n",
        "print(\"First 10 Trigram Counts:\")\n",
        "for trigram, count in trigram_counts.most_common(10):\n",
        "    print(f\"{trigram}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkUrcErkReFG",
        "outputId": "3fe00cee-e1d2-4d23-e88c-ee6bfe37328a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 Trigram Counts:\n",
            "('one', 'of', 'the'): 9716\n",
            "('this', 'movie', 'is'): 5196\n",
            "('this', 'is', 'a'): 4764\n",
            "('a', 'lot', 'of'): 4678\n",
            "('of', 'the', 'film'): 4677\n",
            "('of', 'the', 'movie'): 4069\n",
            "('some', 'of', 'the'): 3754\n",
            "('the', 'film', 'is'): 3650\n",
            "('is', 'one', 'of'): 3542\n",
            "('this', 'film', 'is'): 3457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(last_two_words):\n",
        "    # Split the input to get the last two words\n",
        "\n",
        "    words = last_two_words.lower()\n",
        "    words = words.split()\n",
        "\n",
        "    if len(words) != 2:\n",
        "        print(\"Please provide exactly two words.\")\n",
        "        return None\n",
        "\n",
        "    # Create a prefix from the last two words\n",
        "    prefix = tuple(words)\n",
        "\n",
        "    # Find all trigrams that start with the prefix\n",
        "    possible_trigrams = [(trigram, count) for trigram, count in trigram_counts.items() if trigram[:2] == prefix]\n",
        "\n",
        "    if not possible_trigrams:\n",
        "        print(\"No predictions available for the provided words.\")\n",
        "        return None\n",
        "\n",
        "    # Count occurrences of the third words\n",
        "    third_word_counts = Counter(trigram[2] for trigram, count in possible_trigrams)\n",
        "\n",
        "    # Get the most common third word\n",
        "    most_common_third_word = third_word_counts.most_common(1)[0][0]\n",
        "\n",
        "    return most_common_third_word\n",
        "\n",
        "# Example usage\n",
        "last_two = \"Derrick Cannon\"  # Change this to test with different words\n",
        "predicted_word = predict_next_word(last_two)\n",
        "print(f\"Predicted next word: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqTwcXWqSVL1",
        "outputId": "7043d04c-d844-4720-c31f-6aee164cfd90"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next word: and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wusDmGhBhIRj",
        "outputId": "f85443d0-d94b-4fe3-8f8b-ff81215a0011"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  one of the other reviewers has mentioned that ...  positive\n",
            "1  a wonderful little production the filming tech...  positive\n",
            "2  i thought this was a wonderful way to spend ti...  positive\n",
            "3  basically there's a family where a little boy ...  negative\n",
            "4  petter mattei's \"love in the time of money\" is...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Question\n",
        "\n",
        "\n",
        "\n",
        "train_data = task1file.iloc[:45000]  # First 45,000 rows for training\n",
        "test_data = task1file.iloc[45000:]    # Remaining 5,000 rows for testing\n",
        "\n",
        "reviews_train = train_data['review'].tolist()\n",
        "sentiments_train = train_data['sentiment'].tolist()\n",
        "reviews_test = test_data['review'].tolist()\n",
        "sentiments_test = test_data['sentiment'].tolist()\n",
        "\n",
        "\n",
        "# Assuming 'reviews' is your preprocessed review text and 'labels' contains sentiment\n",
        "all_words = ' '.join(reviews_train).split()\n",
        "vocabulary = set(all_words)\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "\n",
        "\n",
        "label_counts = Counter(sentiments_train)\n",
        "total_reviews = len(sentiments_train)\n",
        "\n",
        "prior_probabilities = {label: count / total_reviews for label, count in label_counts.items()}\n",
        "\n",
        "# Initialize counts for each class\n",
        "likelihoods = {label: Counter() for label in label_counts.keys()}\n",
        "\n",
        "for review, label in zip(reviews_train, sentiments_train):\n",
        "    words = review.split()\n",
        "    likelihoods[label].update(words)\n",
        "\n",
        "# Calculate conditional probabilities with Laplace smoothing\n",
        "conditional_probabilities = {}\n",
        "for label, counts in likelihoods.items():\n",
        "    total_words_in_class = sum(counts.values())\n",
        "    conditional_probabilities[label] = {word: (count + 1) / (total_words_in_class + vocabulary_size)\n",
        "                                        for word, count in counts.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uD8lOaemTPeW"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_review_sentiment(review):\n",
        "    words = review.split()\n",
        "    log_probs = {label: 0 for label in prior_probabilities.keys()}\n",
        "\n",
        "    for label in prior_probabilities.keys():\n",
        "        log_probs[label] = np.log(prior_probabilities[label])  # Start with log of prior\n",
        "\n",
        "        for word in words:\n",
        "            if word in conditional_probabilities[label]:\n",
        "                log_probs[label] += np.log(conditional_probabilities[label][word])\n",
        "            else:\n",
        "                # Use the smoothed probability for unseen words\n",
        "                log_probs[label] += np.log(1 / (sum(likelihoods[label].values()) + vocabulary_size))\n",
        "\n",
        "    # Choose the class with the highest log probability\n",
        "    return max(log_probs, key=log_probs.get)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dZeoaw1Hhddv"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_predictions = 0\n",
        "#for unigram model\n",
        "for review, label in zip(reviews_test, sentiments_test):\n",
        "    predicted_label = predict_review_sentiment(review)\n",
        "    if predicted_label == label:\n",
        "        correct_predictions += 1\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_data['sentiment'], test_data['predicted_sentiment']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp4c6z37h4uJ",
        "outputId": "8f9bba2a-00be-4c05-ee57-413bf1d52f60"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00         8\n",
            "    positive       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        10\n",
            "   macro avg       1.00      1.00      1.00        10\n",
            "weighted avg       1.00      1.00      1.00        10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "# Load your dataset (assumed to be a CSV file with 'review' and 'sentiment' columns)\n",
        "# task1file = pd.read_csv('your_file.csv')\n",
        "\n",
        "# Assume task1file is already preprocessed\n",
        "# Split the data into training and test sets\n",
        "train_size = 49990\n",
        "train_data = task1file[:train_size]\n",
        "test_data = task1file[train_size:]\n",
        "\n",
        "# Create bigrams\n",
        "def create_bigrams(text):\n",
        "    words = text.split()\n",
        "    return [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
        "\n",
        "# Calculate prior and conditional probabilities\n",
        "def calculate_probabilities(data):\n",
        "    total_reviews = len(data)\n",
        "    positive_reviews = data[data['sentiment'] == 'positive']\n",
        "    negative_reviews = data[data['sentiment'] == 'negative']\n",
        "\n",
        "    prior_positive = len(positive_reviews) / total_reviews\n",
        "    prior_negative = len(negative_reviews) / total_reviews\n",
        "\n",
        "    # Bigram counts\n",
        "    bigram_counts_positive = Counter()\n",
        "    bigram_counts_negative = Counter()\n",
        "\n",
        "    # Populate the bigram counts\n",
        "    for review in positive_reviews['review']:\n",
        "        bigrams = create_bigrams(review)\n",
        "        bigram_counts_positive.update(bigrams)\n",
        "\n",
        "    for review in negative_reviews['review']:\n",
        "        bigrams = create_bigrams(review)\n",
        "        bigram_counts_negative.update(bigrams)\n",
        "\n",
        "    # Total bigrams in each class\n",
        "    total_bigrams_positive = sum(bigram_counts_positive.values())\n",
        "    total_bigrams_negative = sum(bigram_counts_negative.values())\n",
        "\n",
        "    # Conditional probabilities with Laplace smoothing\n",
        "    bigram_prob_positive = {bigram: (count + 1) / (total_bigrams_positive + len(bigram_counts_positive))\n",
        "                            for bigram, count in bigram_counts_positive.items()}\n",
        "    bigram_prob_negative = {bigram: (count + 1) / (total_bigrams_negative + len(bigram_counts_negative))\n",
        "                            for bigram, count in bigram_counts_negative.items()}\n",
        "\n",
        "    return prior_positive, prior_negative, bigram_prob_positive, bigram_prob_negative\n",
        "\n",
        "# Function to predict sentiment of a review\n",
        "def predict_sentiment(review, prior_positive, prior_negative, bigram_prob_positive, bigram_prob_negative):\n",
        "    bigrams = create_bigrams(review)\n",
        "    log_prob_positive = np.log(prior_positive)\n",
        "    log_prob_negative = np.log(prior_negative)\n",
        "\n",
        "    # Calculate the log probability for each class\n",
        "    for bigram in bigrams:\n",
        "        log_prob_positive += np.log(bigram_prob_positive.get(bigram, 1 / (sum(bigram_prob_positive.values()) + len(bigram_prob_positive))))\n",
        "        log_prob_negative += np.log(bigram_prob_negative.get(bigram, 1 / (sum(bigram_prob_negative.values()) + len(bigram_prob_negative))))\n",
        "\n",
        "    return 'positive' if log_prob_positive > log_prob_negative else 'negative'\n",
        "\n",
        "# Calculate probabilities from training data\n",
        "prior_positive, prior_negative, bigram_prob_positive, bigram_prob_negative = calculate_probabilities(train_data)\n",
        "\n",
        "# Test the model\n",
        "test_data['predicted_sentiment'] = test_data['review'].apply(\n",
        "    lambda x: predict_sentiment(x, prior_positive, prior_negative, bigram_prob_positive, bigram_prob_negative)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_data['sentiment'], test_data['predicted_sentiment']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGudbRBWigEa",
        "outputId": "6c8c8fd8-f426-4f17-98fc-cbaaf9962b1a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.75      0.86         8\n",
            "    positive       0.50      1.00      0.67         2\n",
            "\n",
            "    accuracy                           0.80        10\n",
            "   macro avg       0.75      0.88      0.76        10\n",
            "weighted avg       0.90      0.80      0.82        10\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-391a5a035f8a>:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['predicted_sentiment'] = test_data['review'].apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset (assumed to be a CSV file with 'review' and 'sentiment' columns)\n",
        "# task1file = pd.read_csv('your_file.csv')\n",
        "\n",
        "# Assume task1file is already preprocessed\n",
        "# Split the data into training and test sets\n",
        "train_size = 49990\n",
        "train_data = task1file[:train_size]\n",
        "test_data = task1file[train_size:]\n",
        "\n",
        "# Create trigrams\n",
        "def create_trigrams(text):\n",
        "    words = text.split()\n",
        "    return [(words[i], words[i + 1], words[i + 2]) for i in range(len(words) - 2)]\n",
        "\n",
        "# Calculate prior and conditional probabilities\n",
        "def calculate_probabilities(data):\n",
        "    total_reviews = len(data)\n",
        "    positive_reviews = data[data['sentiment'] == 'positive']\n",
        "    negative_reviews = data[data['sentiment'] == 'negative']\n",
        "\n",
        "    prior_positive = len(positive_reviews) / total_reviews\n",
        "    prior_negative = len(negative_reviews) / total_reviews\n",
        "\n",
        "    # Trigram counts\n",
        "    trigram_counts_positive = Counter()\n",
        "    trigram_counts_negative = Counter()\n",
        "\n",
        "    # Populate the trigram counts\n",
        "    for review in positive_reviews['review']:\n",
        "        trigrams = create_trigrams(review)\n",
        "        trigram_counts_positive.update(trigrams)\n",
        "\n",
        "    for review in negative_reviews['review']:\n",
        "        trigrams = create_trigrams(review)\n",
        "        trigram_counts_negative.update(trigrams)\n",
        "\n",
        "    # Total trigrams in each class\n",
        "    total_trigrams_positive = sum(trigram_counts_positive.values())\n",
        "    total_trigrams_negative = sum(trigram_counts_negative.values())\n",
        "\n",
        "    # Conditional probabilities with Laplace smoothing\n",
        "    trigram_prob_positive = {trigram: (count + 1) / (total_trigrams_positive + len(trigram_counts_positive))\n",
        "                             for trigram, count in trigram_counts_positive.items()}\n",
        "    trigram_prob_negative = {trigram: (count + 1) / (total_trigrams_negative + len(trigram_counts_negative))\n",
        "                             for trigram, count in trigram_counts_negative.items()}\n",
        "\n",
        "    return prior_positive, prior_negative, trigram_prob_positive, trigram_prob_negative\n",
        "\n",
        "# Function to predict sentiment of a review\n",
        "def predict_sentiment(review, prior_positive, prior_negative, trigram_prob_positive, trigram_prob_negative):\n",
        "    trigrams = create_trigrams(review)\n",
        "    log_prob_positive = np.log(prior_positive)\n",
        "    log_prob_negative = np.log(prior_negative)\n",
        "\n",
        "    # Calculate the log probability for each class\n",
        "    for trigram in trigrams:\n",
        "        log_prob_positive += np.log(trigram_prob_positive.get(trigram, 1 / (sum(trigram_prob_positive.values()) + len(trigram_prob_positive))))\n",
        "        log_prob_negative += np.log(trigram_prob_negative.get(trigram, 1 / (sum(trigram_prob_negative.values()) + len(trigram_prob_negative))))\n",
        "\n",
        "    return 'positive' if log_prob_positive > log_prob_negative else 'negative'\n",
        "\n",
        "# Calculate probabilities from training data\n",
        "prior_positive, prior_negative, trigram_prob_positive, trigram_prob_negative = calculate_probabilities(train_data)\n",
        "\n",
        "# Test the model\n",
        "test_data['predicted_sentiment'] = test_data['review'].apply(\n",
        "    lambda x: predict_sentiment(x, prior_positive, prior_negative, trigram_prob_positive, trigram_prob_negative)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_data['sentiment'], test_data['predicted_sentiment']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RYPsOpMm6tv",
        "outputId": "79e7d284-34b0-47c0-f07b-169c2d01f77e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00         8\n",
            "    positive       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        10\n",
            "   macro avg       1.00      1.00      1.00        10\n",
            "weighted avg       1.00      1.00      1.00        10\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-9209e3e524d9>:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['predicted_sentiment'] = test_data['review'].apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the methods were applied in this assignment. Different approaches were used for the first and the seconde part. The firs part was simple as i caluclated the prbablities of group of words. One for unigram 2 and three for trigram. They were tokenized inn that manner. When getting one word it used the probablities to predict the next word. for unigram the most repeated word was given. For bi and trgrams it looked up the dictionary to see what the most popularr combination was.\n",
        "\n",
        "\n",
        "For the second part different models were trained using the bayseian calculation. For bigrams and trigrams the tokens had their probablity assigned in realtion to whether they appeared in psitiive or negative reviews. When the test set is given it checks where each unigram bigram or trigaram sequence is present in which sentiment the most and calculates the logs to determine the output."
      ],
      "metadata": {
        "id": "FTzJqTUHV1b8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sCcucTTmnswc"
      }
    }
  ]
}